{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df636d2",
   "metadata": {},
   "source": [
    "**Project Automated-Customer Reviews**\n",
    "\n",
    "**- This is the phase 3 of this project**<br>\n",
    "\n",
    "\"<i>\n",
    "Since Gemma-2B is a decoder-only model (like GPT) and is generally used with the \"text-generation\" pipeline, we need to make two key adjustments compared to the BART/FLAN-T5 (encoder-decoder) setup:\n",
    "\n",
    "    Model Name: Change the constant to the Gemma-2B checkpoint.\n",
    "\n",
    "    Pipeline Type: Switch the pipeline in the generation function to \"text-generation\".\n",
    "\n",
    "Here are the updated cells:\n",
    "\n",
    "</i>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b499a",
   "metadata": {},
   "source": [
    "**GEMMA_2B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%1\n",
    "# Imports and Setup (UPDATED)\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import numpy as np \n",
    "\n",
    "# Define constants for file names and the specific AI model\n",
    "SENTIMENT_FILE = 'sentiment_analysis_output.csv'\n",
    "CLUSTERING_FILE = 'clustering_output.csv'\n",
    "\n",
    "# **Update to target the Gemma-2B model**\n",
    "# Note: You may need to run 'huggingface-cli login' in your terminal for Gemma.\n",
    "GEMMA_MODEL = 'google/gemma-2b' \n",
    "MODEL_NAME = GEMMA_MODEL\n",
    "#PAY ATTENTION TO THIS bellow\n",
    "REQUIRED_COLUMNS = [\n",
    "    'review_id', 'product_category', 'product_id', \n",
    "    'review_text', 'sentiment_score', 'meta_category'\n",
    "]\n",
    "\n",
    "print(f\"Setup Complete. Targeting model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb99c0",
   "metadata": {},
   "source": [
    "**Data Loading and prep**<br>#ADD PATHS FOR CSV FILES HERE!!!! BELOW!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531dee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%2\n",
    "# Function to Load and Merge Data\n",
    "\n",
    "def load_and_merge_data():\n",
    "    \"\"\"Loads and merges sentiment and clustering data.\"\"\"\n",
    "    print(\"--- 1. Loading and Merging Data ---\")\n",
    "    try:\n",
    "        #ADD PATHS FOR CSV FILES HERE!!!! BELOW!!!\n",
    "        df_sentiment = pd.read_csv(SENTIMENT_FILE)\n",
    "        df_clustering = pd.read_csv(CLUSTERING_FILE)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please ensure the input CSV files exist.\")\n",
    "        return None\n",
    "\n",
    "    # **NOTE: Adjust the 'on' column name based on your actual data structure.**\n",
    "    df_merged = pd.merge(df_sentiment, df_clustering, on='review_id', how='inner')\n",
    "    \n",
    "    missing_cols = [col for col in REQUIRED_COLUMNS if col not in df_merged.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"ERROR: Merged DataFrame is missing required columns: {missing_cols}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Merged DataFrame shape: {df_merged.shape}\")\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae622bcb",
   "metadata": {},
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835542c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%3\n",
    "# Function to Prepare AI Prompt\n",
    "\n",
    "def prepare_category_input(df_category, min_reviews=10, positive_threshold=0.7, negative_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Analyzes products in a single category and constructs a detailed input prompt \n",
    "    for the generative AI model based on the project requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate Average Sentiment and Identify Top/Worst Products\n",
    "    product_summary = df_category.groupby('product_id')['sentiment_score'].agg(['mean', 'count']).reset_index()\n",
    "    product_summary = product_summary[product_summary['count'] >= min_reviews]\n",
    "    \n",
    "    # Sort and select Top 3 and Worst Product\n",
    "    product_summary = product_summary.sort_values(by='mean', ascending=False)\n",
    "    top_3 = product_summary.head(3)['product_id'].tolist()\n",
    "    \n",
    "    if product_summary.shape[0] < 4:\n",
    "        return None, f\"Only found {product_summary.shape[0]} products with more than {min_reviews} reviews.\"\n",
    "\n",
    "    worst_product = product_summary.tail(1)['product_id'].iloc[0]\n",
    "\n",
    "    # 2. Build the Multi-Part Prompt\n",
    "    category_name = df_category['product_category'].iloc[0]\n",
    "    prompt_sections = [\n",
    "        f\"Generate a compelling, well-structured blog article recommending products in the '{category_name}' category. \"\n",
    "        \"The article MUST detail the Top 3, highlight key differences, list primary complaints for the Top 3, and explain why the Worst Product should be avoided, based ONLY on the provided review data.\"\n",
    "    ]\n",
    "\n",
    "    # --- Section A & B: Top 3 Products, Features, and Complaints ---\n",
    "    \n",
    "    for rank, product_id in enumerate(top_3):\n",
    "        df_product_pos = df_category[(df_category['product_id'] == product_id) & (df_category['sentiment_score'] >= positive_threshold)]\n",
    "        df_product_neg = df_category[(df_category['product_id'] == product_id) & (df_category['sentiment_score'] <= negative_threshold)]\n",
    "        \n",
    "        meta_features = df_product_pos.groupby('meta_category')['review_text'].head(3).str.cat(sep=\" | \").replace('\\n', ' ')\n",
    "        complaint_texts = df_product_neg['review_text'].head(5).str.cat(sep=\" | \").replace('\\n', ' ')\n",
    "        \n",
    "        prompt_sections.append(\n",
    "            f\"\\n\\n--- Product #{rank+1} (Top Rated): {product_id} ---\"\n",
    "            f\"\\nCore Strengths (Clustered Reviews): {meta_features}\"\n",
    "            f\"\\nTop Complaints/Weaknesses: {complaint_texts}\"\n",
    "        )\n",
    "\n",
    "    # --- Section C: Worst Product to Avoid ---\n",
    "    df_worst = df_category[(df_category['product_id'] == worst_product)]\n",
    "    worst_neg_texts = df_worst[(df_worst['sentiment_score'] <= negative_threshold)]['review_text'].head(5).str.cat(sep=\" | \").replace('\\n', ' ')\n",
    "\n",
    "    prompt_sections.append(\n",
    "        f\"\\n\\n--- Product to AVOID: {worst_product} (Lowest Rated) ---\"\n",
    "        f\"\\nKey Negative Reviews/Reasons to Avoid: {worst_neg_texts}\"\n",
    "    )\n",
    "\n",
    "    final_prompt = \"\\n\".join(prompt_sections)\n",
    "    \n",
    "    return category_name, final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4386782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Function for Generative AI (Gemma-2B) - REFORMULATED\n",
    "\n",
    "def generate_article_gemma(prompt):\n",
    "    \"\"\"Uses the specified Gemma-2B model to generate the structured article.\"\"\"\n",
    "    \n",
    "    print(f\"  -> Generating with {MODEL_NAME} (Running on CPU)...\")\n",
    "    \n",
    "    try:\n",
    "        # We use the 'text-generation' pipeline for Gemma.\n",
    "        # Note: Gemma is highly optimized and should be faster on your CPU/RAM than BART or FLAN-T5-Base.\n",
    "        generator = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=MODEL_NAME, \n",
    "            device='cpu'  # Explicitly use CPU\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not load model {MODEL_NAME}. Check installation/path or Hugging Face access.\")\n",
    "        print(f\"  Error details: {e}\")\n",
    "        return \"MODEL LOADING FAILED\"\n",
    "\n",
    "    # Generate the article\n",
    "    result = generator(\n",
    "        prompt, \n",
    "        max_length=700,  # Increased max_length to give Gemma more room\n",
    "        min_length=150, \n",
    "        do_sample=True,  \n",
    "        temperature=0.7,\n",
    "        # Crucial setting for decoder models: Stop generation when a new line or end-of-file is detected in the output\n",
    "        num_return_sequences=1 \n",
    "    )\n",
    "    \n",
    "    # Gemma's output format is different; it returns the entire prompt plus the generated text.\n",
    "    # We must extract only the *newly generated* content.\n",
    "    full_output = result[0]['generated_text']\n",
    "    \n",
    "    # Simple way to extract the generated part by removing the prompt (approximate)\n",
    "    generated_article = full_output.replace(prompt, '').strip()\n",
    "    \n",
    "    # If the replacement fails (due to model paraphrasing the prompt), we return the full output.\n",
    "    return generated_article if generated_article else full_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969963ee",
   "metadata": {},
   "source": [
    "**5: Main Execution Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Main Execution Block (UPDATED FUNCTION CALL)\n",
    "\n",
    "def main_article_generator():\n",
    "    \"\"\"Main function to execute the full template process.\"\"\"\n",
    "    \n",
    "    df_merged = load_and_merge_data()\n",
    "    \n",
    "    if df_merged is None:\n",
    "        return\n",
    "\n",
    "    # Group by the main product category\n",
    "    grouped_by_category = df_merged.groupby('product_category')\n",
    "    \n",
    "    print(\"\\n--- 2. Generating Articles by Category ---\")\n",
    "    \n",
    "    final_articles = {}\n",
    "    \n",
    "    # Iterate over each distinct product category\n",
    "    for category_name, df_category in grouped_by_category:\n",
    "        \n",
    "        print(f\"\\nProcessing Category: {category_name}\")\n",
    "        \n",
    "        # 1. Prepare the highly structured input prompt\n",
    "        category_name, ai_prompt = prepare_category_input(df_category)\n",
    "        \n",
    "        if ai_prompt.startswith(\"Only found\"):\n",
    "            print(f\"  Skipping category: {category_name}. {ai_prompt}\")\n",
    "            continue\n",
    "\n",
    "        # 2. Call the Generative Model\n",
    "        article = generate_article_gemma(ai_prompt) # <-- Function name updated\n",
    "        \n",
    "        final_articles[category_name] = article\n",
    "        \n",
    "        print(f\"âœ… Article Generated for: {category_name}\")\n",
    "\n",
    "    print(\"\\n--- Process Complete. Final Articles: ---\")\n",
    "    \n",
    "    # Final Output Display\n",
    "    for category, article in final_articles.items():\n",
    "        print(f\"\\n=====================================\")\n",
    "        print(f\"GENERATED ARTICLE: {category}\")\n",
    "        print(f\"=====================================\")\n",
    "        print(article)\n",
    "    \n",
    "    return final_articles\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == '__main__':\n",
    "    # Reminder: Cell 2 (load_and_merge_data) and Cell 3 (prepare_category_input) remain unchanged.\n",
    "    generated_articles = main_article_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiagoversion (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
